{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate image to corresponding alphabhet using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pyttsx3\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 50 #We'll be workign with 50 * 50 pixel images\n",
    "\n",
    "LABELS = ['A', 'C', 'E', 'H', 'I', 'L', 'O', 'U', 'V', 'W']\n",
    "\n",
    "engine = pyttsx3.init();\n",
    "engine.setProperty('rate', 105)\n",
    "engine.setProperty('voice', 1)\n",
    "MODEL_PATH = \"withbgmodelv1.h5\"\n",
    "#MODEL_PATH = \"bglessmodelv1.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted letter is E with 93.02003479003906%  confidence\n"
     ]
    }
   ],
   "source": [
    "IMG_PATH = \"test.jpg\"\n",
    "\n",
    "def preprocess_image(IMG_PATH):\n",
    "    \"\"\"\n",
    "    :param IMG_PATH: path of the image\n",
    "    :return: image array by preprocessing the image\n",
    "    Example:\n",
    "        img_array = preprocess_image(\"a.jpg\")\n",
    "    \"\"\"\n",
    "    # Load image with target size and convert img to array\n",
    "    try:\n",
    "        img = cv2.imread(IMG_PATH)\n",
    "        # Change color sapce to gray\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        # Reshape array to l * w * channels\n",
    "        img_array = img.reshape(IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "        # Normalize th array\n",
    "        img_array = img_array / 225.0\n",
    "\n",
    "        # Expand Dimension of the array as our model expects a 4D array\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error\", e)\n",
    "\n",
    "\n",
    "def which_letter(IMG_PATH):\n",
    "    \"\"\"\n",
    "    :param IMG_PATH: path of the image\n",
    "    :return: confident_percent, predicted label using the model or None if exception occurs\n",
    "    eg:\n",
    "        print(which_letter(\"sample.jpeg\"))\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_array = preprocess_image(IMG_PATH)\n",
    "        preds = model.predict(img_array)\n",
    "        preds *= 100\n",
    "        most_likely_class_index = int(np.argmax(preds))\n",
    "        return preds.max(), LABELS[most_likely_class_index]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "conf, label = which_letter(IMG_PATH)\n",
    "print(\"The predicted letter is {} with {}%  confidence\".format(label, conf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted letter is E with 81.69161987304688%  confidence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pre_process(img_array):\n",
    "    \"\"\"\n",
    "    :param img_array: image converted to np array\n",
    "    :return:  img_array after pre-processing(converting to grayscale, resizing, normalizing) the  array\n",
    "    \"\"\"\n",
    "    img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
    "    img_array = cv2.resize(img_array, (50, 50))\n",
    "    # Reshape array to l * w * channels\n",
    "    img_array = img_array.reshape(IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "\n",
    "    # Normalize th array\n",
    "    img_array = img_array / 255.0\n",
    "\n",
    "    # Expand Dimension of the array as our model expects a 4D array\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "def which(img_array):\n",
    "    \"\"\"\n",
    "    :param img_array: np array of image which is to be predicted\n",
    "    :return: confidence precentage and predicted letter\n",
    "    \"\"\"\n",
    "    img_array = pre_process(img_array)\n",
    "    preds = model.predict(img_array)\n",
    "    preds *= 100\n",
    "    most_likely_class_index = int(np.argmax(preds))\n",
    "    return preds.max(), LABELS[most_likely_class_index]\n",
    "\n",
    "img_array = cv2.imread(\"test.jpg\")\n",
    "percent, label = which(img_array)\n",
    "print(\"The predicted letter is {} with {}%  confidence\".format(label, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Frames from Webcam and translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator that uses bgless trained model + segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxContour(contours,  minArea = -1):\n",
    "    maxC = None\n",
    "    maxArea = minArea\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if (area > maxArea):\n",
    "            maxArea = area\n",
    "            maxC = cnt\n",
    "    return maxC\n",
    "\n",
    "\n",
    "def centreCrop(x, y, w, h):\n",
    "    # Centre cropping the image\n",
    "    x_start, y_start, x_end, y_end = 0, 0, 0, 0\n",
    "    mx = 0\n",
    "    if w > 0 and h > 0:\n",
    "        mx = max(w, h, 150)\n",
    "        if x + 7 * mx // 8 > roi_width and y + mx > roi_height:\n",
    "            mx = max(roi_width - x, roi_height - y, 150)\n",
    "            x_start, x_end = roi_width - mx, roi_width\n",
    "            y_start, y_end = roi_height - mx, roi_height\n",
    "\n",
    "        elif y + mx > roi_height and x == 0:\n",
    "            mx = max(w, roi_height - y, 150)\n",
    "            x_start, x_end = x, x + mx\n",
    "            y_start, y_end = roi_height - mx, roi_height\n",
    "        elif y + mx > roi_height:\n",
    "            mx = max(w, roi_height - y, 150)\n",
    "            x_start, x_end = x - mx // 8, x + 7 * mx // 8\n",
    "            y_start, y_end = roi_height - mx, roi_height\n",
    "        elif x + 7 * mx // 8 > roi_width:\n",
    "            mx = max(roi_width - x, h, 150)\n",
    "            x_start, x_end = roi_width - mx, roi_width\n",
    "            y_start, y_end = y, y + mx\n",
    "        elif x == 0:\n",
    "            x_start, x_end = 0, mx\n",
    "            y_start, y_end = y, y + mx\n",
    "        else:\n",
    "            x_start, x_end = x - mx // 8, x + 7 * mx // 8\n",
    "            y_start, y_end = y, y + mx\n",
    "    return  (x_start, y_start), (x_end, y_end)\n",
    "\n",
    "\n",
    "window_name = \"ASL\"\n",
    "frame_height, frame_width, roi_height, roi_width = 480, 900, 600, 300\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)\n",
    "# cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "sentence = \"\"\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret is None:\n",
    "        print(\"No Frame Captured\")\n",
    "        continue\n",
    "\n",
    "    cv2.rectangle(frame, (0, 0), (roi_width, roi_height), (255, 0, 0), 3)  # bounding box which captures ASL sign to be detected by the system\n",
    "\n",
    "    # Crop blue rectangular area(ROI)\n",
    "    img1 = frame[0: roi_height, 0: roi_width]\n",
    "    img_ycrcb = cv2.cvtColor(img1, cv2.COLOR_BGR2YCR_CB)\n",
    "    blur = cv2.GaussianBlur(img_ycrcb, (11, 11), 0)\n",
    "\n",
    "    # lower  and upper skin color\n",
    "    skin_ycrcb_min = np.array((0, 138, 67))\n",
    "    skin_ycrcb_max = np.array((255, 173, 133))\n",
    "\n",
    "    mask = cv2.inRange(blur, skin_ycrcb_min, skin_ycrcb_max)  # detecting the hand in the bounding box\n",
    "\n",
    "    kernel = np.ones((2, 2), dtype = np.uint8)\n",
    "\n",
    "    # Fixes holes in foreground\n",
    "    mask = cv2.dilate(mask, kernel, iterations = 1)\n",
    "\n",
    "    #contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, 2)\n",
    "    _, contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, 2)\n",
    "    cnt = getMaxContour(contours, minArea = 2000)\n",
    "    naya = cv2.bitwise_and(img1, img1, mask = mask)\n",
    "    cv2.imshow(\"mask\", mask)\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "    (x_start, y_start), (x_end, y_end) = centreCrop(x, y, w, h)\n",
    "\n",
    "    # Draw Green Square around the hand\n",
    "    cv2.rectangle(img1, (x_start, y_start), (x_end, y_end), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"naya\", naya)\n",
    "    hand_bg_rm = naya[y_start: y_end, x_start: x_end]\n",
    "    hand = img1[y_start: y_end, x_start: x_end]\n",
    "\n",
    "    # Control Key\n",
    "    c = cv2.waitKey(1) & 0xff\n",
    "\n",
    "    # Speak the sentence\n",
    "    if len(sentence) > 0 and c == ord('s'):\n",
    "        engine.say(sentence)\n",
    "        engine.runAndWait()\n",
    "    # Clear the sentence\n",
    "    if c == ord('c') or c == ord('C'):\n",
    "        sentence = \"\"\n",
    "    # Delete the last character\n",
    "    if c == ord('d') or c == ord('D'):\n",
    "        sentence = sentence[:-1]\n",
    "\n",
    "    # Put Space between words\n",
    "    if c == ord('m') or c == ord('M'):\n",
    "        sentence += \" \"\n",
    "\n",
    "    # If  valid hand area is cropped\n",
    "    if hand.shape[0] != 0 and hand.shape[1] != 0:\n",
    "        #conf, label = which(hand)\n",
    "        conf, label = which(hand_bg_rm)\n",
    "        cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, .7, (0, 0, 255))\n",
    "        if c == ord('n') or c == ord('N'):\n",
    "                sentence += label\n",
    "\n",
    "    cv2.putText(frame, sentence, (50, 70), cv2.FONT_HERSHEY_COMPLEX_SMALL, .7, (0, 0, 255))\n",
    "    cv2.imshow(window_name, frame)\n",
    "    # If pressed ESC break\n",
    "    if c == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator that uses bgless model with fixed ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_name = \"ASL\"\n",
    "frame_height, frame_width, roi_height, roi_width = 480, 900, 200, 200\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)\n",
    "x_start, y_start = 100, 100\n",
    "sentence = \"\"\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret is None:\n",
    "        print(\"No Frame Captured\")\n",
    "        continue\n",
    "\n",
    "    cv2.rectangle(frame, (x_start, y_start), (x_start + roi_width, y_start + roi_height), (255, 0, 0), 3)  # bounding box which captures ASL sign to be detected by the system\n",
    "\n",
    "    # Crop blue rectangular area(ROI)\n",
    "    img1 = frame[y_start: y_start + roi_height, x_start: x_start + roi_width]\n",
    "    img_ycrcb = cv2.cvtColor(img1, cv2.COLOR_BGR2YCR_CB)\n",
    "    blur = cv2.GaussianBlur(img_ycrcb, (11, 11), 0)\n",
    "\n",
    "    # lower  and upper skin color\n",
    "    skin_ycrcb_min = np.array((0, 138, 67))\n",
    "    skin_ycrcb_max = np.array((255, 173, 133))\n",
    "\n",
    "    mask = cv2.inRange(blur, skin_ycrcb_min, skin_ycrcb_max)  # detecting the hand in the bounding box\n",
    "\n",
    "    kernel = np.ones((2, 2), dtype = np.uint8)\n",
    "\n",
    "    # Fixes holes in foreground\n",
    "    mask = cv2.dilate(mask, kernel, iterations = 1)\n",
    "    \n",
    "    naya = cv2.bitwise_and(img1, img1, mask = mask)\n",
    "    cv2.imshow(\"mask\", mask)\n",
    "    cv2.imshow(\"naya\", naya)\n",
    "    hand_bg_rm = naya\n",
    "    hand = img1\n",
    "\n",
    "    # Control Key\n",
    "    c = cv2.waitKey(1) & 0xff\n",
    "\n",
    "    # Speak the sentence\n",
    "    if len(sentence) > 0 and c == ord('s'):\n",
    "        engine.say(sentence)\n",
    "        engine.runAndWait()\n",
    "    # Clear the sentence\n",
    "    if c == ord('c') or c == ord('C'):\n",
    "        sentence = \"\"\n",
    "    # Delete the last character\n",
    "    if c == ord('d') or c == ord('D'):\n",
    "        sentence = sentence[:-1]\n",
    "\n",
    "    # Put Space between words\n",
    "    if c == ord('m') or c == ord('M'):\n",
    "        sentence += \" \"\n",
    "\n",
    "    # If  valid hand area is cropped\n",
    "    if hand.shape[0] != 0 and hand.shape[1] != 0:\n",
    "        conf, label = which(hand_bg_rm)\n",
    "        \n",
    "            \n",
    "        cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, .7, (0, 0, 255))\n",
    "        if c == ord('n') or c == ord('N'):\n",
    "                sentence += label\n",
    "\n",
    "    cv2.putText(frame, sentence, (50, 70), cv2.FONT_HERSHEY_COMPLEX_SMALL, .7, (0, 0, 255))\n",
    "    cv2.imshow(window_name, frame)\n",
    "    # If pressed ESC break\n",
    "    if c == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator that uses with bg model with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxContour(contours,  minArea = -1):\n",
    "    maxC = None\n",
    "    maxArea = minArea\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if (area > maxArea):\n",
    "            maxArea = area\n",
    "            maxC = cnt\n",
    "    return maxC\n",
    "\n",
    "\n",
    "def centreCrop(x, y, w, h):\n",
    "    # Centre cropping the image\n",
    "    x_start, y_start, x_end, y_end = 0, 0, 0, 0\n",
    "    mx = 0\n",
    "    if w > 0 and h > 0:\n",
    "        mx = max(w, h, 150)\n",
    "        if x + 7 * mx // 8 > roi_width and y + mx > roi_height:\n",
    "            mx = max(roi_width - x, roi_height - y, 150)\n",
    "            x_start, x_end = roi_width - mx, roi_width\n",
    "            y_start, y_end = roi_height - mx, roi_height\n",
    "\n",
    "        elif y + mx > roi_height and x == 0:\n",
    "            mx = max(w, roi_height - y, 150)\n",
    "            x_start, x_end = x, x + mx\n",
    "            y_start, y_end = roi_height - mx, roi_height\n",
    "        elif y + mx > roi_height:\n",
    "            mx = max(w, roi_height - y, 150)\n",
    "            x_start, x_end = x - mx // 8, x + 7 * mx // 8\n",
    "            y_start, y_end = roi_height - mx, roi_height\n",
    "        elif x + 7 * mx // 8 > roi_width:\n",
    "            mx = max(roi_width - x, h, 150)\n",
    "            x_start, x_end = roi_width - mx, roi_width\n",
    "            y_start, y_end = y, y + mx\n",
    "        elif x == 0:\n",
    "            x_start, x_end = 0, mx\n",
    "            y_start, y_end = y, y + mx\n",
    "        else:\n",
    "            x_start, x_end = x - mx // 8, x + 7 * mx // 8\n",
    "            y_start, y_end = y, y + mx\n",
    "    return  (x_start, y_start), (x_end, y_end)\n",
    "\n",
    "\n",
    "window_name = \"ASL\"\n",
    "frame_height, frame_width, roi_height, roi_width = 480, 900, 600, 300\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)\n",
    "# cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "sentence = \"\"\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret is None:\n",
    "        print(\"No Frame Captured\")\n",
    "        continue\n",
    "\n",
    "    cv2.rectangle(frame, (0, 0), (roi_width, roi_height), (255, 0, 0), 3)  # bounding box which captures ASL sign to be detected by the system\n",
    "\n",
    "    # Crop blue rectangular area(ROI)\n",
    "    img1 = frame[0: roi_height, 0: roi_width]\n",
    "    img_ycrcb = cv2.cvtColor(img1, cv2.COLOR_BGR2YCR_CB)\n",
    "    blur = cv2.GaussianBlur(img_ycrcb, (11, 11), 0)\n",
    "\n",
    "    # lower  and upper skin color\n",
    "    skin_ycrcb_min = np.array((0, 138, 67))\n",
    "    skin_ycrcb_max = np.array((255, 173, 133))\n",
    "\n",
    "    mask = cv2.inRange(blur, skin_ycrcb_min, skin_ycrcb_max)  # detecting the hand in the bounding box\n",
    "\n",
    "    kernel = np.ones((2, 2), dtype = np.uint8)\n",
    "\n",
    "    # Fixes holes in foreground\n",
    "    mask = cv2.dilate(mask, kernel, iterations = 1)\n",
    "\n",
    "    #contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, 2)\n",
    "    _, contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, 2)\n",
    "    cnt = getMaxContour(contours, minArea = 2000)\n",
    "    naya = cv2.bitwise_and(img1, img1, mask = mask)\n",
    "    cv2.imshow(\"mask\", mask)\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "    (x_start, y_start), (x_end, y_end) = centreCrop(x, y, w, h)\n",
    "\n",
    "    # Draw Green Square around the hand\n",
    "    cv2.rectangle(img1, (x_start, y_start), (x_end, y_end), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"naya\", naya)\n",
    "    hand_bg_rm = naya[y_start: y_end, x_start: x_end]\n",
    "    hand = img1[y_start: y_end, x_start: x_end]\n",
    "\n",
    "    # Control Key\n",
    "    c = cv2.waitKey(1) & 0xff\n",
    "\n",
    "    # Speak the sentence\n",
    "    if len(sentence) > 0 and c == ord('s'):\n",
    "        engine.say(sentence)\n",
    "        engine.runAndWait()\n",
    "    # Clear the sentence\n",
    "    if c == ord('c') or c == ord('C'):\n",
    "        sentence = \"\"\n",
    "    # Delete the last character\n",
    "    if c == ord('d') or c == ord('D'):\n",
    "        sentence = sentence[:-1]\n",
    "\n",
    "    # Put Space between words\n",
    "    if c == ord('m') or c == ord('M'):\n",
    "        sentence += \" \"\n",
    "\n",
    "    # If  valid hand area is cropped\n",
    "    if hand.shape[0] != 0 and hand.shape[1] != 0:\n",
    "        conf, label = which(hand)\n",
    "        #conf, label = which(hand_bg_rm)\n",
    "        cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, .7, (0, 0, 255))\n",
    "        if c == ord('n') or c == ord('N'):\n",
    "                sentence += label\n",
    "\n",
    "    cv2.putText(frame, sentence, (50, 70), cv2.FONT_HERSHEY_COMPLEX_SMALL, .7, (0, 0, 255))\n",
    "    cv2.imshow(window_name, frame)\n",
    "    # If pressed ESC break\n",
    "    if c == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator that uses  bg model without segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_name = \"ASL\"\n",
    "frame_height, frame_width, roi_height, roi_width = 480, 900, 200, 200\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)\n",
    "x_start, y_start = 100, 100\n",
    "sentence = \"\"\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret is None:\n",
    "        print(\"No Frame Captured\")\n",
    "        continue\n",
    "\n",
    "    cv2.rectangle(frame, (x_start, y_start), (x_start + roi_width, y_start + roi_height), (255, 0, 0), 3)  # bounding box which captures ASL sign to be detected by the system\n",
    "\n",
    "    # Crop blue rectangular area(ROI)\n",
    "    img1 = frame[y_start: y_start + roi_height, x_start: x_start + roi_width]\n",
    "    img_ycrcb = cv2.cvtColor(img1, cv2.COLOR_BGR2YCR_CB)\n",
    "    blur = cv2.GaussianBlur(img_ycrcb, (11, 11), 0)\n",
    "\n",
    "    # lower  and upper skin color\n",
    "    skin_ycrcb_min = np.array((0, 138, 67))\n",
    "    skin_ycrcb_max = np.array((255, 173, 133))\n",
    "\n",
    "    mask = cv2.inRange(blur, skin_ycrcb_min, skin_ycrcb_max)  # detecting the hand in the bounding box\n",
    "\n",
    "    kernel = np.ones((2, 2), dtype = np.uint8)\n",
    "\n",
    "    # Fixes holes in foreground\n",
    "    mask = cv2.dilate(mask, kernel, iterations = 1)\n",
    "    \n",
    "    naya = cv2.bitwise_and(img1, img1, mask = mask)\n",
    "    cv2.imshow(\"mask\", mask)\n",
    "    cv2.imshow(\"naya\", naya)\n",
    "    hand_bg_rm = naya\n",
    "    hand = img1\n",
    "\n",
    "    # Control Key\n",
    "    c = cv2.waitKey(1) & 0xff\n",
    "\n",
    "    # Speak the sentence\n",
    "    if len(sentence) > 0 and c == ord('s'):\n",
    "        engine.say(sentence)\n",
    "        engine.runAndWait()\n",
    "    # Clear the sentence\n",
    "    if c == ord('c') or c == ord('C'):\n",
    "        sentence = \"\"\n",
    "    # Delete the last character\n",
    "    if c == ord('d') or c == ord('D'):\n",
    "        sentence = sentence[:-1]\n",
    "\n",
    "    # Put Space between words\n",
    "    if c == ord('m') or c == ord('M'):\n",
    "        sentence += \" \"\n",
    "\n",
    "    # If  valid hand area is cropped\n",
    "    if hand.shape[0] != 0 and hand.shape[1] != 0:\n",
    "        conf, label = which(hand)\n",
    "        \n",
    "            \n",
    "        cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, .7, (0, 0, 255))\n",
    "        if c == ord('n') or c == ord('N'):\n",
    "                sentence += label\n",
    "\n",
    "    cv2.putText(frame, sentence, (50, 70), cv2.FONT_HERSHEY_COMPLEX_SMALL, .7, (0, 0, 255))\n",
    "    cv2.imshow(window_name, frame)\n",
    "    # If pressed ESC break\n",
    "    if c == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
